<!DOCTYPE html>

<html lang="en">

<head>
  <meta charset="utf-8" />

  <title>Blog | ChatAssist</title>
  <link rel="stylesheet" href="normalize.css" />
  <link rel="stylesheet" href="styles.css" />
</head>

<!-- 
    What every member did
    Update on code, links to relevant code added this week
    Update on ideas (and video/pictures)
    Plan for next week
    Blocking issues, help needed
	
	<div class="blog-header">Week :  - , 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">
        <div class="deliverable">
          Deliverable: 
        </div>
        <p>
          <span style="font-size: 1.3em"><strong>Andrew and James</strong></span><br />
      </p>
          <p>
		  
        </p>
        <p>
          <span style="font-size: 1.3em"><strong>Ron and Laurissa:</strong></span><br />
        </p>
        <p>
		
        </p>
		
        </p>
        <span style="font-size: 1.3em"><strong>Update on ideas: </strong></span><br />
        <p>
		
          <ul>
            <li></li>
          </ul>
        </p>
        <span style="font-size: 1.3em"><strong>Current results: </strong></span><br />
        <p>
          Here is the video we made for this week's update presentation. In it, you can see that the chat bubble follows the image being held, and that speaking into the microphone results in the spoken words being displayed in the chat bubble shortly after a pause. This fully meets our goals at the beginning of the project for an MVP. We're really excited to have gotten this working! We've also tested the microphone with a lot of background noise by turning up a street walk Youtube video to max volume and speaking next to the speakers, and the speech detection still works well in that scenario!
        </p>
        <p>
          Here is a hard link in case the embed doesn't work: <a href="https://drive.google.com/file/d/10cmtmWWeUHPfuJ4AtT_5Nd3uNWAx-9rr/preview">https://drive.google.com/file/d/10cmtmWWeUHPfuJ4AtT_5Nd3uNWAx-9rr/preview</a>.
        </p>
        <figure align="center"><iframe width="700" height="500"
            src="https://drive.google.com/file/d/10cmtmWWeUHPfuJ4AtT_5Nd3uNWAx-9rr/preview" frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </figure>
        </p>
        <p>
          <span style="font-size: 1.3em"><strong>Plan for next week:</strong></span><br />
        </p>
        <p>
          Ron and Laurissa will:
          <ul>
            <li></li>
          </ul>
          <br />
          Andrew and James will:
          <ul>
            <li></li>
          </ul>
        </p>

        <p>
          <span style="font-size: 1.3em"><strong>Blocking issues:</strong></span><br />
        </p>
        <p>

        </p>
      </div>




 -->


<body>
  <div class="wrap">
    <div class="header">
      ChatAssist
      <div class="navbar">
        <a class="navbar-entry" href="./index.html">
          Home
        </a>
        <a class="navbar-entry" href="./blog.html">
          Blog
        </a>
        <a class="navbar-entry" href="./team.html">
          Team
        </a>
      </div>
    </div>
    <div class="inner-wrap">

	
	
	  <div class="blog-header">Week 7: May 13 - May 17, 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">
        <div class="deliverable">
          Deliverable: Video, see below
        </div>
        <p>
          <span style="font-size: 1.3em"><strong>Andrew and James</strong></span><br />
      </p>
		  <p>
        <span style="font-size: 1.3em"><strong>Image Tracking <s>2</s> 3: Electric Boogaloo</strong></span> <br />
		  We tested different images for image tracking/anchoring and learned that it was able to track images with very 
      basic features. 
		  </p>
		  <p>
		    <figure align="center">
          <img width=45% src="../chatassist/ChatAssist/Assets/Images/ImageFeature2.png" />
          <img width=45% src="../chatassist/ChatAssist/Assets/Images/ImageFeature1.png" />
          <figcaption>Basic images we tested</figcaption>
        </figure>
		  </p>
		  <p>
		  We find that the the left image performed the best in terms of tracking ability and range. Based on these results, 
      our plan going forward is to utilize the current image tracking and create images with more complex features 
      to maximize the performance as much as possible. In order to do so, these images are planned to be different 
		  variations of a logo for our product. 
		  </p>
		  <p>
		  We attempted to program a QR code reader to track different QR codes and read from them based on something we 
      found off of the Unity Asset Store. As we explored this option, we found that it was infeasible with the time 
      we had and wouldn't perform much better than the built-in image tracking features available on the Magic Leap. 
      We say this because the image tracking is already built in there, so it is probably optimized for the hardware 
      and if the Magic Leap can only do this well with something built for it, it's unlikely that something we hack 
      up over the next few weeks would be much better. We learned that the Magic Leap only allows us to take photos 
      instead of a video stream, so we'd have to take a photo, scan it for multiple QR codes (which QR code readers 
      generally don't do), then realign the anchors. We felt like this would be detrimental to the user experience, 
      as they would have to wait some period of time for the chat bubbles to be realigned. We talked to Alan about 
      these issues and decided that it would be better to spend the next few weeks refining the UI rather than 
      further explore improving the image tracking. Just in case we had extra time, we bought OpenCV for Unity and 
      an example built for the Magic Leap for detecting facial landmarks so that we could improve the anchoring.
		  </p>
      <p>
        <figure align="center">
          <img width=45% src="../chatassist/ChatAssist/Assets/Images/qrcode1.png" />
          <img width=45% src="../chatassist/ChatAssist/Assets/Images/qrcode2.png" />
          <figcaption>QR codes that the Magic Leap couldn't distinguish between</figcaption>
        </figure>
      </p>
		  <p>
		  Laurissa is making the logo for us and we also spent some more time testing the different images like QR codes 
      and seeing how good the Magic Leap's image tracking is. We learned that if we showed it two QR codes, it could 
      not distinguish between the two. We think that this is the case because the features of a QR code are pretty 
      similar, i.e. black edges/squares and white edges/squares. This means that going forward, we need to make sure 
      that the features are fairly distinct and differentiable.
		  </p>
		  
		  <p>
        <span style="font-size: 1.3em"><strong>Video Recording</strong></span><br />
		  We tried to record the video directly off the Magic Leap by testing out the Magic Leap mobile app's beta 
      feature, Device Streaming. We learned that this also uses the camera, so we were unable to record footage using 
      this app. We did this because we were concerned about the final video quality of our project, since the current 
      solution leaves things to be desired. We are going to experiment further with this by bringing a Google Cardboard 
      to put behind the Magic Leap display to see if the video would be more adequate than its current state. 
		  </p>
		  
		  <p>		
        <span style="font-size: 1.3em"><strong>UI Improvements</strong></span><br />
		  James has created a resizable chat bubble so that the bubble will dynamically resize to the amount of text put 
      into the bubble to prevent overflowing. Steve and Ira also suggested making the bubble image transparent so that 
      it doesn't occlude the surroundings. Ira also pointed out that we have some unnecessary things on the UI that 
      we kept for debugging purposes that we should take out, especially since the demo day is coming up. I encountered 
      an error when porting the application to the Magic Leap in that is some jittering/lag that happens when updating 
      the UI via the script connected to the controller. This issue doesn't come up when scrolling (or when receiving 
      text from the server) and it doesn't present any major issues (especially since it doesn't do this when recording 
      the chat log or the speech bubble), but it should still be addressed. I think that it might have something to do 
      with the timer I set up on the controller to send messages to the controller. He plans to move the timer outside 
		  in the future so that there's more modularity in the methods. I'm also going to move the chat log onto the 
      controller so that the user's field of view is less obstructed overall and move it back from the controller a bit 
      more in accordance with Alan's suggestions. I also need to make the scrolling through the language list more intuitive 
		  because the current state of the scrolling is not very accurate. 
		  </p>
		  <p>
		  Steve also suggested that we have some sort of wearable marker, which we had already planned to do in the future, 
      we were planning on doing something that could be clipped on rather than something like a lanyard. We're currently 
      discussing how we should execute this to make it more user-friendly than holding a sheet of paper below their heads. 
		  We're going to discuss this during the weekend because the open house is next week. 
		  </p>
		  <p>
        <span style="font-size: 1.3em"><strong>Internationalization</strong></span> <br />
		  We imported different fonts for Korean, Japanese, and Chinese and integrated them as fallback fonts for the base 
      font we have for the chat bubbles so that we can support more languages in the future. Ron and Laurissa will talk 
      about the support for more languages.
      </p>
      <p>
        <figure align="center">
          <img src="./imgs/language_support.png" />
          <figcaption>Example of language support</figcaption>
        </figure>
      </p>
      <p>
        The code we wrote this week mainly consists of a <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/WebSocketManager.cs">web socket manager</a> (with Ron's help), a <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/LanguageScrollList.cs">
        reduced list of languages</a> with proper encodings for Amazon Transcribe, <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/LanguageScrollList.cs">functions to message the server</a>, and some basic code for <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/smooth.cs">smoothing the transition between locations for parented objects</a>.
      </p>
        <p>
          <span style="font-size: 1.3em"><strong>Ron and Laurissa:</strong></span><br />
          This week, we worked on improving our speech-to-text backend, as well as implementing translation. As before, we worked together in person on every part. However, we could say that Ron worked more on writing the code for Amazon Transcribe/Translate, while Laurissa worked more on setting up the AWS account authentication/credentials and debugging microphone issues on Mac.
        </p>
        <p>
        <span style="font-size: 1.3em"><strong>Amazon Transcribe</strong></span> <br />
        This week, we swapped our backend from Google Cloud to the Amazon Transcribe speech-to-text API.
        We did this because, as discussed in prior blog posts, the Google Cloud Platform Speech-to-Text API
        has a query limit of 1 minute of transcription at a time. It also only provides output on pauses,
        meaning that long sentences don't provide any data to the ChatAssist user until the person is done talking.
          <br/>
          The Amazon Transcribe is more suitable for our needs. It offers up to 4 hours transcription for a single streaming request, meaning that we don't need to hack together some auto-restarting logic that runs every minute. It also provides transcriptions on the fly - if you say a 30 word sentence, it'll start giving you transcriptions only a couple words in. It simply sends further updates on a transcription when it detects the sentence isn't completed yet.
          <br/>
          This is what the transcription looks like:
          <br/>
        <img src="./imgs/transcribe.png"/>
          <br/>
          As you can see, the transcription now sends multiple responses for one spoken sentence. This helps a lot for longer sentences, like the one below:
          <br/>
        <img src="./imgs/transcribe-long.png"/>
          <br/>
          The code for this is more complicated than before. We now have to gracefully handle multiple responses on one sentence. We are still working on improving the code here, as we are running into latency issues when trying to translate all the responses we get. We will probably need some way to heuristically drop out some of the responses (if we detect longer transcriptions of the same text have already been received). We also want to avoid outputting some of the inaccurate detections - these frequently happen when Transcribe detects half a spoken word as a full word, like "say so" or "pause it" in the above screenshot. We're not sure if it's possible - we'll have to look more at the API to see if we can get some sort of confidence score about the transcription.
          <br/>
          Amazon Transcribe does seem slightly less accurate than GCP Speech-to-Text. However, we believe it's worth the tradeoff of losing the 1 minute restriction and long sentence issues. It's good enough to demo, and we can say it's Amazon's responsibility to make a more accurate product haha.

        </p>

        <p>
        <span style="font-size: 1.3em"><strong>Amazon Translate</strong></span> <br />
        We also set up translation this week. In the backend, the server now tracks each MagicLeap client. Each client can then request a specific language, and all requests to that client will now be translated before being sent. This introduces a minor amount of latency per request - however, when combined with the rapid partial responses from Transcribe, we are encountering latency issues, where text is detected but not sent to the MagicLeap for a couple seconds due to translation requests stalling the main thread. This will be our primary focus to fix next week.
        <br/>
        We first tried using the Google Translate API. Unfortunately, we ran into a mysterious 403 "Query limit exceeded" error that we were unable to resolve. Apparently, lots of other people have this issue as well, since there were a ton of StackOverflow posts about it. We decided to just use Amazon Translate instead, since neural net architectures for machine translation at this point are pretty standard and any big service provider probably uses near state-of-the-art models so there's no reason to think Amazon's offering is any worse. Amazon Translate was much easier to set up, and we have it working now with automatic detection of a source language, and user selection of a target language. If there is no translation needed, the query is not made (which reduces latency).
        <br/>
        Here is a screenshot of our server translating an English message received from a mic to German and forwarding it to the MagicLeap.
        <img src="./imgs/translate.png"/>
        </p>
		
        </p>
        <span style="font-size: 1.3em"><strong>Update on ideas: </strong></span><br />
        <p>
		
          <ul>
            <li></li>
          </ul>
        </p>
        <span style="font-size: 1.3em"><strong>Current results: </strong></span><br />
        <p>
		  Here's a video of us demoing our translation from English to German on the bubble.
        </p>
        <p>
          Here is a hard link in case the embed doesn't work: <a href="https://drive.google.com/file/d/1hFi5NNre3yu277flu2kmQ9Sb00YpArKI/view">https://drive.google.com/file/d/1hFi5NNre3yu277flu2kmQ9Sb00YpArKI/view</a>.
        </p>
        <figure align="center"><iframe src="https://drive.google.com/file/d/1hFi5NNre3yu277flu2kmQ9Sb00YpArKI/preview" width="640" height="480"></iframe>
        </figure>
        </p>
        <p>
          <span style="font-size: 1.3em"><strong>Plan for next week:</strong></span><br />
        </p>
		Generally, get ready for the open house:
        <p>
          Ron and Laurissa will:
          <ul>
            <li></li>
          </ul>
          <br />
          Andrew and James will:
          <ul>
            <li>Process language change requests by sending to server</li>
			<li>Testing out how much space we will need for the demo (before Tuesday)</li>
			<li>Adjust offsets and UI to improve user experience</li>
			<li>Test out transparency of the text and depending on results, add controls for changing opacity</li>
          </ul>
        </p>

        <p>
          <span style="font-size: 1.3em"><strong>Blocking issues:</strong></span><br />
        </p>
        <p>
			<li>We're not sure about how to isolate the voices of the speaker for the microphones, i.e. decreasing microphone sensitivity. </li>
			<li>We need more people to test this so that we can get feedback on our project. </li>
			<li>Amazon Transcribe's transcription is not very accurate, but we can't exactly fix this, and it might be possibly worse for other languages.</li>
        </p>
      </div>
	
	
      <div class="blog-header">Week 6: May 6 - May 10, 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">
        <div class="deliverable">
          Deliverable: <a
            href="https://docs.google.com/presentation/d/1-ahWr0YxFX5gXWFuWkPIt9mwZaGf7DaV7nHO_27TEtM/edit?usp=sharing">Mid-project
            update presentation</a> and video (under 'Current results')
        </div>
        <p>
          <span style="font-size: 1.3em"><strong>Andrew and James</strong></span><br />
      </p>
          <p>
          <p>
             <strong>Implementing Image Tracking</strong>
          </p>
          James and Andrew met up over the weekend to combine the image tracking scene that had the server with the UI from a
          week ago. We first created the image tracking scene from our first scene that we create with built-in image tracking based 
          on the example provided from Magic Leap. After that, we spent a few hours testing out differently-sized images for 
          tracking. We used a small printed-out version of the original larger image that we have in the demo video and it worked 
          decently. The main problem we had with that was that even covering a small portion of the image gave the Magic Leap a hard
          time detecting the image, which isn't ideal because we want the end-user to wear some small image that won't
          get in the way of the conversation. We think that this happens because image tracking/detection is essentially
          based on feature-detection. A lot of features are based on edges/corners, so it's possible that we were
          covering up some important features for detection.
          <br />
          We're going to try detecting a smaller version of the image as the base for what we're trying to detect so
          that we have a better idea of what features the image we are going to use for our final product should have.
          We'll also test out newer images and we're going to revisit the QR code idea because QR codes have clearly
          defined features. We spent some time on Tuesday after the class testing out microphones. We were
          there earlier and we came upon a weird bug that moved where the chat bubble was generated around 2000 times
          further than what we coded. We couldn't find out a reason for why it did that because the code for setting the
          position wasn't complicated, but it still did that. Eventually, it fixed itself after we restarted Unity. Our group as a whole 
          spent some time trying to get the bluetooth microphone to work. From what we could find online, it seems like
          it was initially meant for amplifiers and speakers rather than to be used as input for a computer. We were
          able to get it to run on some websites, but when we try to access it on our application, it kind of just died
          and wouldn't respond. We also looked for some information on the microphone, but we couldn't find anything
          relevant on the internet.
          <br />
          Andrew created the initial outline for the presentation and then we all added parts and information about the
          things that we worked on. Tuesday night, he was informed that the scene that we had that had all the components
          was broken and that some scripts became unset on some of the components, which was strange because it was
          working when Andrew left the labs. This was why the UI didn't show up in the demo video. He ended up coming in and
          fixed it on Wednesday, but he still wonders as to how that happened. Our current hypothesis for why that
          happened was that we accidentally changed the metadata in some way and it just became lost. We presented on
          Thursday and we left early to start the blog post for the week.
          <br />
          The code we wrote this week mainly consists of a <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Prefabs/ChatBubbleLauncher.prefab">new chat bubble launcher prefab</a>, 
          <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scenes/Combined.unity">the combined unity scene with Andrew's chat language selection UI and James's chat bubbles</a>, <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/Visualizers/BubbleVisualizer.cs">a new bubble visualizer script</a>, <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/MultiBubbleVisualizer.cs">an attempt at a multiple bubble visualizer script</a>, <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/EdgeScrollChatLog.cs">an updated chat log scrolling script</a>, <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/ImageTracking.cs">an image tracking driver script</a>, <a href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/ImageTrackingViz.cs"> and finally an image tracking visualizer script</a>.
        </p>
        <figure align="center">
          <img src="./imgs/current_setup.PNG" />
          <figcaption>Fig.4 - Current diagram of setup minus servers/API calls</figcaption>
        </figure>
        <p>
          <span style="font-size: 1.3em"><strong>Ron and Laurissa:</strong></span><br />
        </p>
        <p>
          Like in previous weeks, we worked on all of the following progress/code together in person. We both were
          involved in every part, but if we had to assign one person to each section then we would say Ron worked
          slightly more on the streaming APIs and microphone setup, while Laurissa worked slightly more on setting up
          the microphone client.
          <br />
          Most of the code we wrote this week is in <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/tree/master/apis/micclient">apis/micclient</a>
          and <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/tree/master/apis/speechserver">apis/speechserver</a>.
        </p>
        <p>
          <strong>Streaming Speech-to-Text</strong>
        </p>
        <p>
          Our first order of business was figuring out streaming speech-to-text. In prior weeks, we had set up
          speech-to-text for recorded audio. For our application to work smoothly, we need speech-to-text to work live,
          which means it must transcribe an open audio stream. To handle this, we use GCP's Streaming Speech-to-Text
          API, which requires us to set up an AudioInputStream in Java. We then set up a response observer that receives
          transcripts from GCP.
          <br />
          The way the API works is that it reads the audio stream and automatically detects pauses. When it detects a
          pause, it does a transcription and sends the transcript to the API caller. This means that you do have to kind
          of force pauses into a conversation for there to be transcriptions, but we've found it to be fairly
          reasonable. The latency is also quite good - we're able to get and display the transcript within a second of a
          pause in speech.
          <br />
          There is one major challenge to overcome here: the GCP streaming API has a hard limit of 60 seconds. It cuts
          off transcriptions after this 60 second limit. This is not good for our application - if you're in the middle
          of speaking, that audio is not transcribed at all. We are planning to explore workarounds, including
          potentially swapping API providers. A preliminary readthrough of docs suggests that AWS has a streaming time
          limit of 4 hours, which is much more suitable for our purposes. However, it supports fewer spoken languages.
          <br />
          The streaming transcription looks something like this:
          <br />
          <img src="./imgs/mic-transcribe.png" />
        </p>

        <p>
          <strong>Microphone Client</strong>
        </p>
        <p>
          We set up the microphone client this week. The microphone client is a Java process that does three things:
          <ol>
            <li>The client connects to an audio input (i.e. microphone) on the computer and retrieves an audio stream
              from that input.</li>
            <li>The client connects the stream to a speech-to-text API call.</li>
            <li>The client listens for responses from the API and forwards them to the central server.</li>
          </ol>
          Internally, the microphone client creates a WebSocket client that connects to the central server address. It
          then sets up an audio stream and feeds that to the GCP API, establishing a listener for responses which
          forwards the responses to the central server via the WebSocket.
          <br />
          We modified the central server so that it now looks for messages from microphone clients and forwards them to
          connected MagicLeaps. Our current design is very close to fully supporting multiple microphones - we just need
          to add an identifier to each client and have the server and MagicLeap handle the identifiers correctly.
          <br />
          Below is an example of what the server log looks like - you can see text being received from an IP
          corresponding to the microphone client. The disconnection at the end happens because of the 60 second time
          limit on GCP API calls that we discussed above.
          <br />
          <img src="./imgs/server.png" />
        </p>

        <p>
          <strong>Microphone Setup</strong>
        </p>
        <p>
          It turns out microphones are pretty tricky to work with as audio streams. We ordered two microphones from
          Amazon, one wireless and one wired. Thus far, we have been unable to get the wireless microphone to work,
          although we'll continue trying different options. Our computers literally just don't detect it at all. We have
          gotten the wired microphone working on some of our computers, which is what we use for our demo. Our code selects the correct dataline to access the microphone and then opens an audio stream reading from it.
          <br/>
          Here is the wireless microphone we bought: <a href="https://www.amazon.com/XIAOKOA-Microphone-Transmission-Amplifier-Recording/dp/B01HOB5SKE/">https://www.amazon.com/XIAOKOA-Microphone-Transmission-Amplifier-Recording/dp/B01HOB5SKE/</a>
          <br />
          The reviews are quite good, so we're surprised that it's been such a struggle to set it up. Our wired mic is a cheap lavalier microphone that seems to be working alright out of the box.
          <br />
          We are considering buying another microphone to test with, since it's important to us that we have a working
          set of mics. Ideally, for the demo we'll be able to have three mic clients running with wireless microphones.
        </p>
        <span style="font-size: 1.3em"><strong>Update on ideas: </strong></span><br />
        <p>
          We don't have any major updates on ideas - our vision is still pretty close to what we described in our PRD. A couple clarifying ideas are:
          <ul>
            <li>We may need a different approach for tracking</li>
            <li>We may need to switch to a different API due to challenges from GCP's 60-second streaming restriction</li>
            <li>We will probably pick a couple supported spoken languages and have more languages available to translate to</li>
          </ul>
        </p>
        <span style="font-size: 1.3em"><strong>Current results: </strong></span><br />
        <p>
          Here is the video we made for this week's update presentation. In it, you can see that the chat bubble follows the image being held, and that speaking into the microphone results in the spoken words being displayed in the chat bubble shortly after a pause. This fully meets our goals at the beginning of the project for an MVP. We're really excited to have gotten this working! We've also tested the microphone with a lot of background noise by turning up a street walk Youtube video to max volume and speaking next to the speakers, and the speech detection still works well in that scenario!
        </p>
        <p>
          Here is a hard link in case the embed doesn't work: <a href="https://drive.google.com/file/d/10cmtmWWeUHPfuJ4AtT_5Nd3uNWAx-9rr/preview">https://drive.google.com/file/d/10cmtmWWeUHPfuJ4AtT_5Nd3uNWAx-9rr/preview</a>.
        </p>
        <figure align="center"><iframe width="700" height="500"
            src="https://drive.google.com/file/d/10cmtmWWeUHPfuJ4AtT_5Nd3uNWAx-9rr/preview" frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </figure>
        </p>
        <p>
          <span style="font-size: 1.3em"><strong>Plan for next week:</strong></span><br />
        </p>
        <p>
          Ron and Laurissa will:
          <ul>
            <li>Figure out a solution to the streaming 60-second-limit problem</li>
            <li>Set up support for multiple mic clients</li>
            <li>Set up a translation API to display spoken text in different languages</li>
            <li>Experiment with detecting spoken text in different languages (French and Spanish)</li>
          </ul>
          <br />
          Andrew and James will:
          <ul>
            <li>Test out smaller-sized images as the base image for tracking</li>
            <li>Revisit the QR code idea</li>
            <li>Translate the set language to something that the speech-to-text API can interpret</li>
            <li>Cut down the number of supported languages to speed up language detection and for easier testing</li>
            <li>Send messages from the MagicLeap to the server for the choice of language</li>
          </ul>
        </p>

        <p>
          <span style="font-size: 1.3em"><strong>Blocking issues:</strong></span><br />
        </p>
        <p>
          We don't have any major blocking issues. Our primary concerns right now are tracking (built-in image tracking
          seems to be pretty awful) and the streaming limits, but we have promising ideas for solving both of these
          problems.
        </p>
      </div>




      <div class="blog-header">Week 5: April 29 - May 3, 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">
        <div class="deliverable">
          Deliverable: Demo as seen below
        </div>
        <p>
          <span style="font-size: 1.3em"><strong>Andrew:</strong></span><br />
          This week, I made the UI that we presented last week better and integrated the Magic Leap controller into the
          project. I added a menu for the user to select the language they want the speech to be translated to,
          following the feedback for the PRD, and added an initial chat log that we had during our project proposal. The
          scene lets users scroll through a list of languages on the left and go through a log of messages on the right
          and allows for easy integration with other parts of the project by having public methods that can be called to
          get the language the user selects and append to the chat log.
          <br />
          I took the feedback from last week about not having static things on the screen because that obstructs the
          field of view of the user and it feels unnatural to have something follow you everywhere as you move your
          head, so the boxes sort of float into place and we let the user hide the language selection and chat logs by
          scrolling to the center. I added a scroll feature by getting the direction the user swipes on the controller
          and made the language selection and chat logs only visible if the user scrolls towards it. The current
          iteration of the scrolling is a little janky because it reads off the gestures on the touchpad, which aren't
          entirely accurate.
          <br />
          I also tested adding text to the chat log by adding text on the down trigger to make sure that text can be
          added. Ideally, the chat log will allow easier support for multiple speakers by distinguishing who said what
          when as it'll maintain a transcript of the conversation. If we can't figure out how to anchor the text to some
          image, we will still be able to display the text on the chat log; but we will work to avoid this by getting
          the image recognition to work for the anchoring.
          <br />
          <figure align="center">
            <img src="./imgs/initial_ui.png" />
            <figcaption>Fig.2 - Magic Leap Week 5 ChatAssist UI</figcaption>
          </figure>
          <br />
          Alan gave some feedback saying that we should probably consider hooking the menu up to the controller because
          that's what people have been doing nowadays with menus in AR/VR. I didn't know this and I plan on working on
          that in the future, but I need to focus on getting the anchoring down for the demo next week. I also wanted to
          include a video of the language selection and the chat log, but the Magic Leap was running low on battery and
          the controller got unpaired. I somehow missed the last resort in the instructions for how to pair it again, so
          I couldn't get it to pair and I just put it back into the locker and charged it. James later fixed this.
          <br />
          The code for this work is in the following links: <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/ChatList.cs">appending
            to the chat log</a>, <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/ControllerSelection.cs">controller
            integration</a>, <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/EdgeScrollChatLog.cs">scrolling
            in chat log</a>, <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/LanguageScrollList.cs">adding
            language selection and scrolling</a>, and the <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/LanguageToggle.cs">toggle
            for the languages</a>.
        </p>
        <p>
          <span style="font-size: 1.3em"><strong>James:</strong></span><br />
          This week, I made the UI for displaying the chat bubbles better than the demo that we presented last week. I
          added some
          scripts to the code, as well as created prefabs of the speech-to-text bubbles so that we will be able to
          generate as many
          of these bubbles as we want. The scene currently presents four bubbles generated by the script in real space
          in four directions around the user. These bubbles always are able to face the user so that the text
          transcribed is readable
          i.e. not flipped in the opposite way. I had some issues early on with the structure of the prefab, as I had it
          originally
          as a two-part design where I had the bubbles as a separate prefab, and I had an additional prefab for the
          canvas that will
          generate these bubbles, but they were not able to display. After discussing the issue with John, I changed the
          design so that there will be a single empty game object in the scene that will automatically generate these
          bubble prefabs, and that
          prefab contains a Canvas, image sprite of the chat bubble, and the TextMesh Pro text inside to give the
          display. With the
          feedback from last week to not have static objects on the screen, the chat bubbles as they are right now are a
          step in the
          right direction, as you can move around and have the bubbles not be in sight anymore.
          <br />
          More development needs to be done on how much text we should display in the chat bubble, either through
          automatically resizing the bubble when text is generated, adding some user interaction to the bubble for them
          to resize the bubbles and expand them, changing look of chat bubble via custom image, and etc., but the
          following tasks that we have layed out below for next week are more important at the moment.
          <br />
          I also collaborated with Ron and Laurissa to hook up and test the server code that they made with the bubble
          generation
          scripts so that these bubbles will be able to display the text from that code in roughly real time. More
          details and a
          video of the demo are presented below in the code integration section.
          <br />
          The code for this work is in the following links: <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Prefabs/SpeechToTextCanvas.prefab">prefab
            for the chat bubble</a>, <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/tree/master/chatassist/ChatAssist/Assets/Images">images
            for chat bubbles</a>, <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/PanelGen.cs">script
            for generating the bubbles</a>, <a
            href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/SpeechToText.cs">script
            for producing the text in the bubble</a>.
        </p>
        <p>
          <strong>On Image Tracking</strong>
          <p>
            <p>
              We started testing out the Magic Leap's image recognition examples last week, but were unable to get the
              examples to work, thinking that we needed physical copies of the images. We tested it again after class on
              Thursday using images on the computer and printed copies on sheets of paper and met more success, getting
              the demo to work on some preset images.
              <br />
              We tried doing the same with some solid-color images and tried to replace the images to see if they could
              detect new images right off the bat. The fastest path to a working demo is to take the code they have and
              the images they recognize and use those as the basis of our method for anchoring. This will give us a
              working product for the demo and we'd be done.
              <br />
              But we want to at least look through the code and see what can be done with it, like push the boundaries
              of the size of the image so that people don't have to wear 8.5" by 11" sheets of paper to get this to
              work. We will continue exploring this on Friday, the weekend, and days leading up to the demo day. We also
              plan on meeting with Alan on Friday.
            </p>
            <span style="font-size: 1.3em"><strong>Ron and Laurissa:</strong></span><br />
            This week, we had a number of technical challenges which took a long time to figure out, but managed to come
            up with and build a functional workaround (and plan for next steps) after understanding what the problems
            were. We continued to work together on this part, since our work required a lot of reading various
            documentation, finding solutions to obscure problems, and choosing the correct libraries to use, that was
            more
            effective with two people searching and discussing design decisions.
            <br />
            Specifically though, although we sat and wrote all the code in person together so we both wrote code for
            both
            parts, we can roughly ascribe the C#/Unity-side stuff to Laurissa, and the Java WebSocket server and Google
            Cloud Platform (GCP) API setup to Ron.
            <br />
            Now we will discuss the various challenges we had in more detail, and the end results we were able to
            produce.
          </p>
          <p>
            <strong>Running GCP code on MagicLeap / Unity</strong>
          </p>
          <p>
            As a reminder, last week we wrote working API calls to GCP in a generic C# solution in Visual Studio. This
            week, we found out that just having C# code does not mean it'll run on Unity. Unity actually runs Mono, a
            cross-platform version of C#, whereas Visual Studio uses Visual C#, which is *slightly* different.
            Unfortunately, these differences are enough to make it really difficult to run arbitrary C# libraries on
            Unity.
            <br />
            Basically, things do not compile out of the box. It's possible to make it work, but pretty complicated. We
            tried
            a variety of approaches. For example, we tried just copying the GCP packages over to Packages in the Unity
            project, but it turns out that you need to make the library you want into a Unity package, which also means
            bundling all its dependencies. Otherwise, Unity may occasionally clear out the Packages folder on its own as
            it regenerates the solution. We also tried using NuGet with Unity, but couldn't get the configurations to
            work. In the meantime, we also found out that the MagicLeap does not necessarily support the C# APIs used in
            the GCP codebase.
            <br />
            At one point we also looked at the Amazon AWS API. Unfortunately, their Unity-specific API does not support
            speech-to-text, so we would probably run into the same issues trying to use their .NET API.
            <br />
            We decided at this point that while we could invest a lot more time (already took us many hours to exhaust
            the
            different approaches) in getting GCP queries to run on MagicLeap by building it as a Unity package, it might
            make sense to have a standalone server handling those queries. This design choice also makes sense from a
            realistic perspective: consumer apps will typically not make API calls directly, since that would involve
            exposing the private API key to the client. Instead, most apps that use cloud APIs will send requests from
            clients to a hosted server and then run the actual queries through that server. So, it's totally reasonable
            for us to follow a similar approach.
            <br />
            Here is what we are envisioning now <strong>(we worked out this design as a team of 4,</strong> Ron made it
            in
            Powerpoint):
            <br />
            <figure align="center">
              <img src="./imgs/design.png" />
              <figcaption>Fig.3 - Overall Infrastructure Design</figcaption>
            </figure>
            <br />
            <strong>Conclusion:</strong> GCP code is hard to run on MagicLeap. Instead, we've decided to go with a
            server-based approach.
          </p>
          <p>
            <strong>Setting up the client and server</strong>
          </p>
          <p>
            We decided to use WebSockets to handle client-server connections, since they seem appropriate for the
            long-held socket connections that we want to have for constantly displaying text.
            <br />
            Once again, WebSockets do not exist in native Unity code. Unity has networking code, but it does not play
            too
            well with non-Unity servers. We looked around at various libraries, and found that MagicLeap actually didn't
            support WebSockets at all until recently - a couple months ago - in the Lumin SDK 0.20 version that we're
            using. We found that websocket-sharp seems to suit our purposes and was confirmed to work on MagicLeap.
            <br />
            To get the library to be on Unity, we had to build it as a DLL since there is no prebuilt solution for
            Windows, which we are developing on. This meant we had to set up the MonoDevelop IDE, since websocket-sharp
            was written in Mono (which is why it plays well with Unity). After that, we were able to build a DLL file
            containing the library, and we put it in Assets/Plugins in our Unity project.
            <br />
            Once we had the DLL file working, it was just a couple of lines to get a working WebSocket client in
            Unity/MagicLeap that would connect to a WebSocket server and be able to send and receive messages.
            <br />
            We then put together a WebSocket server. We decided to use Java for this, since this is the language we're
            most familiar with since we will need to write some more networking code in the near future. We also had to
            rewrite the code for using the GCP speech-to-text API in Java.
            <br />
            At this point, we have a fully functioning connection between the MagicLeap and our server. The MagicLeap
            can
            send arbitrary binary strings to the server, and the server can respond with a binary string. This covers
            all
            the networking functionality we'll need betwen the MagicLeap and non-MagicLeap things.
            <br />
            <strong>Conclusion:</strong> We have fully working networking between the MagicLeap client and a server now!
            The MagicLeap can receive text from the server and display it. The server is getting this text by making a
            call to GCP. Next week the server will actually receive the text from other microphone clients that will
            individually making calls to GCP. We've found the latency is low enough to be totally acceptable.
          </p>
          <p>
            The code for our work is mostly in the <a
              href="https://github.com/UWRealityLab/vrcapstone19sp-team3/tree/master/apis">apis</a> folder. We also
            hooked up our code in ChatAssist's <a
              href="https://github.com/UWRealityLab/vrcapstone19sp-team3/blob/master/chatassist/ChatAssist/Assets/Scripts/SpeechToText.cs">SpeechToText.cs</a>
            script.
          </p>
          <p>
            <strong>Integrating our code</strong>
          </p>
          <p>
            We worked together as a team of 4 for this part. Basically, we've now tied our code together so that the
            MagicLeap is able to show arbitrary text sent to it from the server. For the sake of this mini demo we also
            wrote some code on our server to arbitrarily change the text to a user input, although that functionality
            isn't actually needed since the microphone clients will update the server with text data.
            <br />
            Here is a video showing what our project now looks like (should be embedded below but we've also put the
            link <a href="https://www.youtube.com/embed/bW56ecAwUH4" target="_blank">here</a>):
            <br />
            <br />
            <figure align="center"><iframe width="700" height="500" src="https://www.youtube.com/embed/bW56ecAwUH4"
                frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe></figure>
          </p>
          <p>
            <span style="font-size: 1.3em"><strong>Plan for next week:</strong></span><br />
          </p>
          <p>
            We will continue to integrate our code for our demo on Thursday. Specifically...
            <br />
            Ron and Laurissa will:
            <ul>
              <li>Get the streaming speech-to-text API to work (fairly complex, streams are tough to work with)</li>
              <li>Use a microphone with the streaming API</li>
              <li>Write the microphone client and make it forward data to the server</li>
              <li>Make the server send data to the MagicLeap in a structured fashion</li>
              <li>Update the MagicLeap / Unity code to accept text updates in a structured fashion</li>
            </ul>
            We haven't totally decided how we'll split this work, but probably Ron will work on the streaming API and
            the basic microphone client setup, and Laurissa will work on linking that to the current server and
            structuring the data in some consistent way.
            <br />
            Andrew and James will:
            <ul>
              <li>Test the Magic Leap's image recognition capabilities</li>
              <li>Hack up a version of anchoring using the Magic Leap examples</li>
              <li>Determine where we want to generate the messages relative to the the anchor</li>
              <li>Determine what unique images we want to detect</li>
            </ul>
            More specifically Andrew will look into the Magic Leap image tracking code and determine the extent in which
            we can use the code base and James will look into the Magic Leap image tracking example to hack up a
            rudimentary version of anchoring.
          </p>
      </div>

      <div class="blog-header">Week 4: April 22 - 26, 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">

        <div class="deliverable">
          Deliverable: <a href="./index.html"> Updated PRD (on Home page)</a>
        </div>

        <p>
          Ron and Laurissa:
          This week, we tested different Speech-To-Text APIs including Microsoft Azure, Amazon Transcribe,
          and Google Cloud. We tested them out by using our phone as a microphone since we haven't bought
          a proper one yet. At first, we thought that the room was too loud to pick up the exact words
          we're saying and it made us think about how when we demo our final product on our demo day,
          it might have the same amount of noise. After some trial and error, we found out that it only works
          for .flac audio files. In the example, they provided a .flac file which worked pretty well. We
          learned that we have to convert our mp3 file to a .flac file so that it is compatible with it.
          We decided to use Google's Cloud Speech-to-Text API because it supports widest
          variety of languages and contains different feature that we found useful. Some of which includes
          the language detection and multispeaker content.
        </p>

        <p>
          Andrew and James:
          We also tested different ways of generating some preliminary chat bubbles with some text in Unity.
          Right now, they are simply objects within Unity that have a fixed text in them. From what we saw,
          the text itself is a bit visible, but the visual itself still has much to be desired in terms of
          presentation to the user with respect to other objects.
          The text boxes are aligned with the camera, which can be good in some contexts with respect to our UI menu
          for setting the language of the user, as it keeps the bubble in the view of the user, but it still needs some
          more work to be better fleshed out.
          What was currently visible by the user is this...
          <figure align="center">
            <img src="imgs/week4_2.jpg" alt="Magic Leap Week 4 ChatAssist Display">
            <figcaption>Fig.1 - Magic Leap Week 4 ChatAssist Display</figcaption>
          </figure>

        </p>

        <p>
          Our current plan for next week is to order some wireless clip-on microphones so that we can test and
          we also want to get the Speech-To-Text API up and running on the Magic Leap. Additionally, we will develop
          the pipeline to generate text that can be seen by the user, play around with some of the Magic Leap examples,
          and
          develop the chat bubbles more into an actual product which can display text effectively to the user. We will
          also look
          into a way to anchor the bubble to something in the real world, ideally something that can be held by a
          person.
          We will also look into some chat box assets as well to make them look more appealing compared to what they are
          now.
          The overall goal for next week is to combine the work from our two teams to make a soft demo of ChatAssist.
        </p>

        <div class="references">
          Some useful references for this week:
          <a href="https://www.youtube.com/watch?v=SxYvufk7Hrs&feature=youtu.be">Perfect Text in Unity</a>
          <a href="https://assetstore.unity.com/search/?k=chat+box&order_by=relevance&q=chat&q=box&rows=42">Possible
            Chat Box Assets</a>
        </div>
      </div>

      <div class="blog-header">Week 3: April 15 - 19, 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">

        <div class="deliverable">
          Deliverable: <a href="./index.html">PRD (on Home page)</a>
        </div>

        <p>
          This week, we gave our project pitch in class where we talked about the problem
          we are addressing with our product, the technologies that we are planning to use,
          and a timeline of what we plan to accomplish each week. Aditya suggested that we
          use the spatial mapping mesh to physically drop the chat bubble if we were unable
          to use cameras to find some unique marker. After class, we decided to get microphones
          so that we can have more options than just relying on our devices' microphones.
        </p>
        <p>
          We completed our Product Requirements Document (PRD), which details our
          project plan with deliverables, features, performance metrics, milestones,
          responsibilities of each team member, materials and budget, risks, and how
          risks will be addressed. This can be found at the top of the 'Home' tab.
        </p>
        <p>
          We worked on the slides and PRD as a group. In the lab session, we set up
          the Magic Leap and ran into some trouble when working through the guides. We
          eventually got this cleared up and learned that we could only run our applications
          on two of the 4 computers we were given because the Magic Leap is a very
          resource-intensive device. We learned what we needed to do with the Magic Leap and
          that we needed to update it when it's charged so that it can run the applications
          we built using Unity.
        </p>
        <p>
          Our current plan for next week is to have Ron and Laurissa test out different APIs
          for speech-to-text translation and have James and Andrew display some readable
          text on the Magic Leap.
        </p>
        <p>
          One of the problems that we faced this week is that the Magic Leap gets very hot, but
          Alan told us that we should probably just close all the applications when it's not
          being used in order to make it less likely to overheat. This may prove to make the
          device less user-friendly, since people probably don't want something hot on their
          heads, so we're wondering how other people who develop on the Magic Leap deal with this.
        </p>

        <div class="references">
          Some useful references for this week:
          <a href="https://creator.magicleap.com/learn/guides/unity-setup">Setting up Unity Projects on Magic Leap</a>
          <a href="https://docs.google.com/document/d/1tEB1_UU1wg3c1qH75UlhwRIBhAvC4alidN_7Bcb5AyY/edit">Magic Leap
            set-up instructions</a>
        </div>
      </div>


      <div class="blog-header">Week 1 + 2: April 1 - 12, 2019</div>
      <div class="horizontal-divider"></div>
      <div class="blog-entry">
        <!-- 
    What every member did
    Update on code, links to relevant code added this week
    Update on ideas
    Plan for next week
    Blocking issues, help needed
 -->
        <div class="deliverable">
          Deliverable: <a href="./project_proposal.pdf">Project Proposal</a>
        </div>

        <p>
          This week, we all got together to discuss possible ideas. In class,
          we've been working on the various tutorials. We also all worked
          together on the website and project proposal.
        </p>
        <p>
          The code we wrote this week was for this website and for the
          tutorials we did in class.
        </p>
        <p>
          Originally, we thought of doing some kind of game-like simulation,
          but we weren't able to come up with a clear direction on the purpose
          of the simulation. Right now, our idea is to build a conversation
          tool that displays chat bubbles over people's heads as they speak
          and records the conversation text. It would use speech-to-text tech
          and the built-in microphones to figure out where words are coming
          from. This would be useful for people who need clear transcriptions
          of speech. For example, it could benefit people with impaired
          hearing, people who don't speak a language natively, or people in
          business meetings who want a log of the conversation.
        </p>
        <p>
          Next week, we will prepare our team pitch and figure out what
          resources and assets we need to make this project a reality.
        </p>
        <p>
          Currently, we're not stuck on anything. We're excitedly waiting to
          finish our pitch so we can start doing real work on our project!
        </p>

        <div class="references">
          Some useful references for this week:
          <a
            href="https://www.reddit.com/r/magicleap/comments/85w7tn/faq/">https://www.reddit.com/r/magicleap/comments/85w7tn/faq/</a>
          <a href="https://www.youtube.com/watch?v=GItvRqmuUME">https://www.youtube.com/watch?v=GItvRqmuUME</a>
        </div>

      </div>

    </div>
  </div>
  </div>
  </div>
</body>

</html>